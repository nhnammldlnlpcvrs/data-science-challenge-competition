{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc # cleaning memory\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm # tracking training process\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Paths\n",
    "BASE_DIR   = \"/content/drive/MyDrive/Colab Notebooks/data-science-challenge-competition\"\n",
    "DATA_DIR   = os.path.join(BASE_DIR, \"data\")\n",
    "MODEL_DIR  = os.path.join(BASE_DIR, \"model\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"result\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "best_path = os.path.join(MODEL_DIR, \"best_overall_model.pt\")\n",
    "submit_path = os.path.join(RESULT_DIR, \"submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191530f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366efdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data (JSONL)\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_records = read_jsonl(os.path.join(DATA_DIR, \"processed/vihallu-train-split.jsonl\"))\n",
    "val_records   = read_jsonl(os.path.join(DATA_DIR, \"processed/vihallu-val-split.jsonl\"))\n",
    "test_records  = read_jsonl(os.path.join(DATA_DIR, \"jsonl/vihallu-public-test.jsonl\"))\n",
    "\n",
    "train_df = pd.DataFrame(train_records).fillna({\"context\":\"\", \"prompt\":\"\", \"response\":\"\"})\n",
    "val_df   = pd.DataFrame(val_records).fillna({\"context\":\"\", \"prompt\":\"\", \"response\":\"\"})\n",
    "df_test  = pd.DataFrame(test_records).fillna({\"context\":\"\", \"prompt\":\"\", \"response\":\"\"})\n",
    "\n",
    "# Combine train and val for K-Fold Cross Validation\n",
    "full_train_df = pd.concat([train_df, val_df], ignore_index=True).fillna({\"context\":\"\", \"prompt\":\"\", \"response\":\"\"})\n",
    "df_test = df_test.fillna({\"context\":\"\", \"prompt\":\"\", \"response\":\"\"})\n",
    "\n",
    "# --- Label Mapping ---\n",
    "labels = sorted(full_train_df['label'].unique().tolist())\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "full_train_df['label'] = full_train_df['label'].map(label2id)\n",
    "\n",
    "print(f\"Total training data size for K-Fold: {len(full_train_df)}\")\n",
    "print(f\"Test data size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Cleans GPU memory before starting training.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Hyperparameters\n",
    "\n",
    "class CFG:\n",
    "    fusion_type = \"concat\"  # options: \"concat\", \"bilinear\", \"gated\"\n",
    "    freeze_backbone = False  # nếu muốn thử chỉ train các head\n",
    "    \n",
    "    # Model & Tokenizer\n",
    "    model_name = \"uitnlp/CafeBERT\"\n",
    "    max_length = 512\n",
    "\n",
    "    # Training\n",
    "    batch_size = 24\n",
    "    gradient_accumulation_steps = 2\n",
    "    num_train_epochs = 15\n",
    "    fp16 = True\n",
    "    early_stopping_patience = 3\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.05\n",
    "\n",
    "    # Loss Weights\n",
    "    alpha = 0.7\n",
    "    beta = 0.3\n",
    "\n",
    "    # Model Architecture\n",
    "    latent_dim = 384\n",
    "    dropout_rate = 0.3\n",
    "    bilstm_hidden_size = 384\n",
    "    bilstm_num_layers = 1\n",
    "    transformer_num_heads = 8\n",
    "    transformer_num_layers = 2\n",
    "\n",
    "    # K-Fold\n",
    "    n_splits = 5\n",
    "    seed = 42\n",
    "\n",
    "# --- Initialize Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.model_name,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3086ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class (With Pseudo-label)\n",
    "\n",
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.contexts = df['context'].values\n",
    "        self.prompts = df['prompt'].values\n",
    "        self.responses = df['response'].values\n",
    "        if 'label' in df.columns:\n",
    "            self.labels = df['label'].values\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = str(self.contexts[idx])\n",
    "        prompt = str(self.prompts[idx])\n",
    "        response = str(self.responses[idx])\n",
    "\n",
    "        # Context: 0, prompt+response: 1\n",
    "        encoding = self.tokenizer(\n",
    "            context,\n",
    "            prompt + \" \" + response,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten()\n",
    "        }\n",
    "\n",
    "        if self.labels is not None:\n",
    "            real_label = self.labels[idx]\n",
    "            item['real_label'] = torch.tensor(real_label, dtype=torch.long)\n",
    "\n",
    "            if real_label == label2id['no']: pseudo_label_nli = 0\n",
    "            elif real_label == label2id['intrinsic']: pseudo_label_nli = 1\n",
    "            else: pseudo_label_nli = 2\n",
    "\n",
    "            pseudo_label_coverage = 0.0 if real_label == label2id['extrinsic'] else 1.0\n",
    "\n",
    "            item['pseudo_label_nli'] = torch.tensor(pseudo_label_nli, dtype=torch.long)\n",
    "            item['pseudo_label_coverage'] = torch.tensor(pseudo_label_coverage, dtype=torch.float)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Task Hallucination  Model\n",
    "\n",
    "class HallucinationModel(nn.Module):\n",
    "    def __init__(self, model_name, cfg):\n",
    "        super(HallucinationModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        bert_hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        if cfg.freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.bilstm = nn.LSTM(\n",
    "            bert_hidden_size * 4,\n",
    "            cfg.bilstm_hidden_size,\n",
    "            cfg.bilstm_num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.bilstm_hidden_size * 2, # d_model phải khớp với output của BiLSTM\n",
    "            nhead=cfg.transformer_num_heads,\n",
    "            dim_feedforward=cfg.bilstm_hidden_size * 2 * 4,\n",
    "            dropout=cfg.dropout_rate,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.nli_transformer_encoder = nn.TransformerEncoder(\n",
    "            transformer_layer,\n",
    "            num_layers=cfg.transformer_num_layers\n",
    "        )\n",
    "\n",
    "        self.mlp_nli = nn.Sequential(\n",
    "            nn.Linear(cfg.bilstm_hidden_size * 2 * 2, cfg.latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg.dropout_rate)\n",
    "        )\n",
    "        self.nli_classifier = nn.Linear(cfg.latent_dim, 3)\n",
    "\n",
    "        self.mlp_coverage = nn.Sequential(\n",
    "            nn.Linear(2, cfg.latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg.dropout_rate)\n",
    "        )\n",
    "        self.coverage_regressor = nn.Linear(cfg.latent_dim, 1)\n",
    "\n",
    "        self.mlp_norm_nli = nn.LayerNorm(cfg.latent_dim)\n",
    "        self.mlp_norm_coverage = nn.LayerNorm(cfg.latent_dim)\n",
    "\n",
    "        self.bilinear = nn.Bilinear(cfg.latent_dim, cfg.latent_dim, cfg.latent_dim)\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(cfg.latent_dim * 3, cfg.latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg.dropout_rate),\n",
    "            nn.Linear(cfg.latent_dim, len(label2id))\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        H_all_tokens = outputs.hidden_states[-1]\n",
    "\n",
    "        # --- Tách H_context và H_response một cách hiệu quả ---\n",
    "        # Tạo mặt nạ từ token_type_ids\n",
    "        # context_mask có giá trị 1 ở vị trí context, 0 ở nơi khác\n",
    "        context_mask = (token_type_ids == 0) & (attention_mask == 1)\n",
    "        # response_mask có giá trị 1 ở vị trí response, 0 ở nơi khác\n",
    "        response_mask = (token_type_ids == 1) & (attention_mask == 1)\n",
    "\n",
    "        # Mở rộng mặt nạ để nhân với H_all_tokens\n",
    "        # Kích thước: (batch_size, seq_len, 1)\n",
    "        context_mask_expanded = context_mask.unsqueeze(-1).float()\n",
    "        response_mask_expanded = response_mask.unsqueeze(-1).float()\n",
    "\n",
    "        # Dùng phép nhân để \"zero-out\" các token không liên quan\n",
    "        H_context = H_all_tokens * context_mask_expanded\n",
    "        H_response = H_all_tokens * response_mask_expanded\n",
    "\n",
    "        # NLI Head\n",
    "        e = torch.matmul(H_context, H_response.transpose(1, 2))\n",
    "        tilde_H_context = torch.matmul(torch.softmax(e, dim=2), H_response)\n",
    "        M_context = torch.cat([H_context, tilde_H_context, H_context - tilde_H_context, H_context * tilde_H_context], dim=-1)\n",
    "\n",
    "        context_lengths = context_mask.sum(dim=1).cpu()\n",
    "        context_lengths = torch.clamp(context_lengths, min=1)\n",
    "        packed_M_context = pack_padded_sequence(M_context, context_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_V_context, _ = self.bilstm(packed_M_context)\n",
    "        V_context, _ = pad_packed_sequence(packed_V_context, batch_first=True, total_length=self.cfg.max_length)\n",
    "\n",
    "        src_key_padding_mask = (context_mask == 0)\n",
    "        V_prime_context = self.nli_transformer_encoder(V_context, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Pooling\n",
    "        sum_pooled = torch.sum(V_prime_context, 1)\n",
    "        count_pooled = context_mask.sum(1, keepdim=True)\n",
    "        avg_pool = torch.where(count_pooled > 0, sum_pooled / (count_pooled + 1e-9), torch.zeros_like(sum_pooled))\n",
    "        V_prime_context_masked = V_prime_context.masked_fill(context_mask.unsqueeze(-1).logical_not(), -1e4)\n",
    "        max_pool, _ = torch.max(V_prime_context_masked, 1)\n",
    "\n",
    "        latent_nli_vector = self.mlp_nli(torch.cat((avg_pool, max_pool), 1))\n",
    "        latent_nli_vector = self.mlp_norm_nli(latent_nli_vector) # <--- NORM NLI\n",
    "        logits_nli = self.nli_classifier(latent_nli_vector)\n",
    "\n",
    "        # Coverage head\n",
    "        safe_mask_value = -1e4\n",
    "\n",
    "        context_token_count = context_mask.sum(1, keepdim=True)\n",
    "        response_token_count = response_mask.sum(1, keepdim=True)\n",
    "\n",
    "        # Forward score\n",
    "        e_masked_forward = e.masked_fill(response_mask.unsqueeze(1).logical_not(), safe_mask_value)\n",
    "        align_scores_forward, _ = torch.max(e_masked_forward, dim=2)\n",
    "        sum_scores_forward = torch.sum(align_scores_forward, dim=1, keepdim=True)\n",
    "        forward_score = torch.where(context_token_count > 0, sum_scores_forward / (context_token_count + 1e-9), torch.zeros_like(sum_scores_forward))\n",
    "\n",
    "        # Backward score\n",
    "        e_masked_backward = e.masked_fill(context_mask.unsqueeze(2).logical_not(), safe_mask_value)\n",
    "        align_scores_backward, _ = torch.max(e_masked_backward, dim=1)\n",
    "        sum_scores_backward = torch.sum(align_scores_backward, dim=1, keepdim=True)\n",
    "        backward_score = torch.where(response_token_count > 0, sum_scores_backward / (response_token_count + 1e-9), torch.zeros_like(sum_scores_backward))\n",
    "\n",
    "        latent_coverage_vector = self.mlp_coverage(torch.cat([forward_score, backward_score], dim=1))\n",
    "        latent_coverage_vector = self.mlp_norm_coverage(latent_coverage_vector)\n",
    "        predicted_score_coverage = self.coverage_regressor(latent_coverage_vector)\n",
    "\n",
    "        # Fusion\n",
    "        if self.cfg.fusion_type == \"bilinear\":\n",
    "            fusion_part = self.fusion(latent_nli_vector, latent_coverage_vector)\n",
    "            fused_vector = torch.cat([latent_nli_vector, latent_coverage_vector, fusion_part], dim=1)\n",
    "        elif self.cfg.fusion_type == \"gated\":\n",
    "            gate_val = torch.sigmoid(self.gate(torch.cat([latent_nli_vector, latent_coverage_vector], dim=1)))\n",
    "            fused_vector = gate_val * latent_nli_vector + (1 - gate_val) * latent_coverage_vector\n",
    "        else:  # concat\n",
    "            fused_vector = torch.cat([latent_nli_vector, latent_coverage_vector], dim=1)\n",
    "\n",
    "        final_logits = self.final_classifier(fused_vector)\n",
    "\n",
    "        return {\"final_logits\": final_logits, \"logits_nli\": logits_nli, \"predicted_score_coverage\": predicted_score_coverage.squeeze(-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, cfg, loss_fns):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        real_labels = batch['real_label'].to(device)\n",
    "        pseudo_nli_labels = batch['pseudo_label_nli'].to(device)\n",
    "        pseudo_cov_labels = batch['pseudo_label_coverage'].to(device)\n",
    "\n",
    "        with autocast(enabled=cfg.fp16):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            loss_classifier = loss_fns['classifier'](outputs['final_logits'], real_labels)\n",
    "            logits_nli_fp32 = outputs['logits_nli'].float()\n",
    "            pred_score_cov_fp32 = outputs['predicted_score_coverage'].float()\n",
    "            loss_nli = loss_fns['nli'](logits_nli_fp32, pseudo_nli_labels)\n",
    "            loss_coverage = loss_fns['coverage'](pred_score_cov_fp32, pseudo_cov_labels)\n",
    "            loss = loss_classifier + cfg.alpha * loss_nli + cfg.beta * loss_coverage\n",
    "\n",
    "        loss = loss / cfg.gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * cfg.gradient_accumulation_steps\n",
    "        progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# --- Cập nhật hàm eval_model ---\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            preds = torch.argmax(outputs['final_logits'], dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actuals.extend(batch['real_label'].cpu().numpy())\n",
    "\n",
    "    # Tính toán các chỉ số\n",
    "    macro_f1 = f1_score(actuals, predictions, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(actuals, predictions, labels=list(id2label.keys()))\n",
    "\n",
    "    # --- TẠO RA BÁO CÁO DẠNG CHUỖI ---\n",
    "    report_str = classification_report(\n",
    "        actuals,\n",
    "        predictions,\n",
    "        target_names=labels,\n",
    "        zero_division=0,\n",
    "        digits=4 # Thêm 4 chữ số thập phân cho chi tiết\n",
    "    )\n",
    "\n",
    "    # Trả về báo cáo dạng chuỗi\n",
    "    return macro_f1, cm, report_str, actuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state = None\n",
    "best_overall_f1 = -1\n",
    "best_val_idx = None # Lưu lại index của tập val tốt nhất\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "# Vòng lặp chính cho K-Fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(full_train_df, full_train_df['label'])):\n",
    "    print(f\"==================== FOLD {fold+1}/{CFG.n_splits} ====================\")\n",
    "    clean_memory() # Dọn dẹp VRAM trước khi bắt đầu một fold mới\n",
    "\n",
    "    # 1. Chia dữ liệu, tính weights, tạo Dataloader\n",
    "    train_fold_df = full_train_df.iloc[train_idx]\n",
    "    val_fold_df = full_train_df.iloc[val_idx]\n",
    "\n",
    "    label_counts = train_fold_df['label'].value_counts().sort_index()\n",
    "    class_weights = (len(train_fold_df) / (len(labels) * label_counts)).values\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    print(f\"Calculated Class Weights: {class_weights_tensor.cpu().numpy()}\")\n",
    "\n",
    "    train_dataset = HallucinationDataset(train_fold_df, tokenizer, CFG.max_length)\n",
    "    val_dataset = HallucinationDataset(val_fold_df, tokenizer, CFG.max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # 2. Khởi tạo mô hình và các thành phần huấn luyện\n",
    "    model = HallucinationModel(CFG.model_name, CFG).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "    num_training_steps = len(train_loader) * CFG.num_train_epochs // CFG.gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(num_training_steps * 0.1),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.fp16)\n",
    "\n",
    "    loss_fns = {\n",
    "        'classifier': nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1),\n",
    "        'nli': nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "        'coverage': nn.MSELoss()\n",
    "    }\n",
    "\n",
    "    # 3. Vòng lặp huấn luyện cho từng epoch\n",
    "    best_fold_f1 = -1\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(CFG.num_train_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{CFG.num_train_epochs} ---\")\n",
    "\n",
    "        avg_train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, device, CFG, loss_fns)\n",
    "\n",
    "        epoch_duration_secs = time.time() - start_time\n",
    "        print(f\"Epoch duration: {str(timedelta(seconds=epoch_duration_secs))}\")\n",
    "\n",
    "        # --- Báo cáo chi tiết cuối Epoch ---\n",
    "        macro_f1, cm, report_str, val_actuals, val_preds = eval_model(model, val_loader, device)\n",
    "\n",
    "        print(f\"  -> Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  -> Macro-F1: {macro_f1:.4f}\")\n",
    "        print(\"\\nClassification Report:\\n\", report_str)\n",
    "\n",
    "        # Trực quan hóa và đóng biểu đồ ngay lập tức\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Confusion Matrix - Fold {fold+1} Epoch {epoch+1}')\n",
    "        plt.show()\n",
    "        plt.close() # <-- ĐÓNG FIGURE ĐỂ GIẢI PHÓNG BỘ NHỚ\n",
    "\n",
    "        # --- Logic của Early Stopping và lưu model tốt nhất ---\n",
    "        if macro_f1 > best_fold_f1:\n",
    "            best_fold_f1 = macro_f1\n",
    "            patience_counter = 0\n",
    "            if best_fold_f1 > best_overall_f1:\n",
    "                best_overall_f1 = best_fold_f1\n",
    "                print(f\"BEST MODEL FOUND! Saving state and val_idx...\")\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                best_val_idx = val_idx # Lưu lại val_idx tương ứng\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in this fold. Patience: {patience_counter}/{CFG.early_stopping_patience}\")\n",
    "            if patience_counter >= CFG.early_stopping_patience:\n",
    "                print(\"Early stopping for this fold!\")\n",
    "                break\n",
    "\n",
    "        # --- DỌN DẸP CUỐI EPOCH (LOGIC ĐÚNG) ---\n",
    "        del macro_f1, cm, report_str, val_actuals, val_preds, avg_train_loss\n",
    "        clean_memory()\n",
    "        # ----------------------------------------\n",
    "\n",
    "    # --- Dọn dẹp cuối FOLD ---\n",
    "    del model, optimizer, scheduler, scaler, loss_fns, train_loader, val_loader, train_dataset, val_dataset\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n==================== K-FOLD TRAINING COMPLETE ====================\")\n",
    "\n",
    "# --- ĐÁNH GIÁ MÔ HÌNH TỐT NHẤT TỪ BIẾN ĐÃ LƯU ---\n",
    "if best_model_state is not None:\n",
    "    print(f\"The best overall model achieved a Macro-F1 of {best_overall_f1:.4f} during training.\")\n",
    "    print(\"\\n--- Evaluating the best overall model on its corresponding best validation set ---\")\n",
    "\n",
    "    # 1. Dọn dẹp VRAM trước khi tải mô hình mới để đánh giá\n",
    "    clean_memory()\n",
    "\n",
    "    # 2. Khởi tạo một kiến trúc mô hình mới và tải state_dict (trọng số) đã lưu vào\n",
    "    best_model = HallucinationModel(CFG.model_name, CFG).to(device)\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "\n",
    "    # 3. Tạo DataLoader từ tập validation TƯƠNG ỨNG với mô hình tốt nhất\n",
    "    # Sử dụng `best_val_idx` đã được lưu lại trong quá trình training\n",
    "    best_val_df = full_train_df.iloc[best_val_idx]\n",
    "    final_val_dataset = HallucinationDataset(best_val_df, tokenizer, CFG.max_length)\n",
    "    final_val_loader = DataLoader(\n",
    "        final_val_dataset,\n",
    "        batch_size=CFG.batch_size * 4, # Có thể dùng batch size lớn hơn khi đánh giá\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # 4. Thực hiện đánh giá chỉ một lần\n",
    "    final_macro_f1, final_cm, final_report_str, _, _ = eval_model(best_model, final_val_loader, device)\n",
    "\n",
    "    # 5. In ra các kết quả chi tiết cuối cùng\n",
    "    print(f\"\\nRe-evaluated Macro-F1 on its best fold's validation set: {final_macro_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nFinal Classification Report:\\n\")\n",
    "    print(final_report_str)\n",
    "\n",
    "    # 6. Trực quan hóa Confusion Matrix cuối cùng\n",
    "    print(\"\\nFinal Confusion Matrix:\\n\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(final_cm, annot=True, fmt='d', cmap='Greens', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Final Confusion Matrix on Best Model (F1: {final_macro_f1:.4f})')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(\"\\nTraining finished, but no best model was saved in memory (best_model_state is None).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-challenge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
